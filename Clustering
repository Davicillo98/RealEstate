#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jun 13 18:23:25 2021

@author: davicillo
"""
#%% IMPORTAR DEPENDENCIAS

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
import xgboost as xgb
from sklearn.neural_network import MLPRegressor




#%% 0. CARGAR DATOS

df = pd.read_csv(r'/Users/davicillo/Desktop/InvestMat/Segundo Semestre/REDES/Listings_Clean.csv')
columns = df.columns.tolist()
df = df.drop(['detailedType',
              'suggestedTexts', 
              'parkingSpace', 
              'url',
              'has3DTour',
              'has360',
              'showAddress',
              'latitude',
              'longitude',
              'municipality',
              'operation',
              'province',
              'priceByArea'], axis=1)




#%% 1. RELLENAR VALORES EN BLANCO Y LIMPIEZA

df_na = (df.isnull().sum() / len(df)) * 100
df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' : df_na})
missing_data

df["parkingSpacePrice"] = df["parkingSpacePrice"].fillna("None")
df["subTypology"] = df["subTypology"].fillna("NA")
df["newDevelopmentFinished"] = df["newDevelopmentFinished"].fillna("NA")
df["isParkingSpaceIncludedInPrice"] = df["isParkingSpaceIncludedInPrice"].fillna("False")
df["hasParkingSpace"] = df["hasParkingSpace"].fillna("False")
df["hasLift"] = df["hasLift"].fillna("False")
df["floor"] = df["floor"].fillna("bj")
df["floor"].replace({"BJ": "bj", "ss": "st"}, inplace=True)
df["floor"].replace({"st": "-1"}, inplace=True)
df["floor"].replace({"en": "0"}, inplace=True)
df["floor"].replace({"bj": "0"}, inplace=True)




#%% 2. TRANSFORMACIÃ“N DE VALORES

# Fixing skewed features
df["price"] = np.log1p(df["price"])


# Check all numerical skewed features
numeric_feats = df.dtypes[(df.dtypes == "int64")|(df.dtypes == "float64")].index
skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
skewness = pd.DataFrame({'Skewed Features' :skewed_feats})
skewness.head()

skewness = skewness[abs(skewness) > 0.75].dropna()
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))


from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    df[feat] = boxcox1p(df[feat], lam)
    df[feat] += 1


# Process columns and apply LabelEncoder to categorical features
objList = df.select_dtypes(include = ["object", "bool"]).columns
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for feat in objList:
    df[feat] = le.fit_transform(df[feat].astype(str))
    
    
to_cluster = df.drop(columns=['price', 'district', 'neighborhood'])




#%% 3. TEST SCORES FOR KMEANS

from sklearn.cluster import KMeans
import numpy as np
from sklearn import metrics
wcss = []
silhouette_sc = []
davies_sc = []
calinski_sc = []

range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
for n_clusters in range_n_clusters:
    clusterer = KMeans(n_clusters = n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(to_cluster)
    
    wcss_avg = clusterer.inertia_    
    silhouette_avg = metrics.silhouette_score(to_cluster, cluster_labels)
    davies_avg = metrics.davies_bouldin_score(to_cluster, cluster_labels)
    calinski_avg = metrics.calinski_harabasz_score(to_cluster, cluster_labels)
    
    wcss.append(wcss_avg)
    silhouette_sc.append(silhouette_avg)
    davies_sc.append(davies_avg)
    calinski_sc.append(calinski_avg)


# WCSS (lower the better)
plt.style.use('default')
plt.plot(range_n_clusters, wcss)
plt.ylabel('Inertia Score')
plt.xlabel('Number of clusters')
plt.show()

# Silhouette (higher the better)
plt.plot(range_n_clusters, silhouette_sc)
plt.ylabel('Silhouette Score')
plt.xlabel('Number of clusters')
plt.show()

# Davies-Bouldin (lower the better)
plt.plot(range_n_clusters, davies_sc)
plt.ylabel('Davies-Bouldin Score')
plt.xlabel('Number of clusters')
plt.show()

# Calinski-Harabasz (higher the better)
plt.plot(range_n_clusters, calinski_sc)
plt.ylabel('Calinski-Harabasz Score')
plt.xlabel('Number of clusters')
plt.show()
    
    
    
    
#%% 4. KMEANS INITIALISATION

kmeans = KMeans(n_clusters=3, random_state=0).fit(to_cluster)
labels = kmeans.predict(to_cluster)

df = df.drop(columns = ['Clusters'])
df.insert(0, 'Clusters', labels)
df.Clusters.value_counts()




#%% 5. PREDICCIONES Y MODELADO

X = df.drop(['price'], axis=1)
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = 0.2, 
                                                    random_state = 123)


# MLP
model_mlp = MLPRegressor(hidden_layer_sizes = (50,50,50),
                         activation = 'tanh',
                         alpha = 0.0001,
                         solver = 'adam',
                         learning_rate = 'constant',
                         random_state = 1, 
                         max_iter = 500).fit(X_train, y_train)


y_pred = model_mlp.predict(X_test)
r2_score(y_test, y_pred)


y_test_inv = np.expm1(y_test)
y_pred_inv = np.expm1(y_pred)


mean_squared_error(y_test, y_pred)
mean_absolute_percentage_error(y_test_inv, y_pred_inv)
mean_absolute_error(y_test_inv, y_pred_inv)



# XGB
model_xgb = xgb.XGBRegressor(colsample_bytree=0.2, 
                             gamma=0.01, 
                             learning_rate=0.05, 
                             max_depth=4, 
                             min_child_weight=1.5, 
                             n_estimators=7200,
                             reg_alpha=0.9, 
                             reg_lambda=0.6,
                             subsample=0.2,
                             seed=42, 
                             random_state =7).fit(X_train, y_train)


y_pred = model_xgb.predict(X_test)
y_true = model_xgb.predict(X_train)
r2_score(y_test, y_pred)


y_test_inv=np.expm1(y_test)
y_pred_inv=np.expm1(y_pred)
mse = mean_squared_error(y_test_inv, y_pred_inv)
mean_absolute_percentage_error(y_test_inv, y_pred_inv)
mean_absolute_error(y_test_inv, y_pred_inv)


